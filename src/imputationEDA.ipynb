{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledge that the dataset herein is miniature and can be toyed with on Jupyter nb using a local machine, perhaps with single GPU. More realistic datasets would require API interfacing with a cluster and data engineering pipelines to display distributions of billions of params over time, or to run robust imputation methods for missing data as standalone endeavors before one even considers modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/train_sparked.csv\")\n",
    "X_test_nulls_raw = pd.read_csv(\"../data/test_sparked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SellerCity</th>\n",
       "      <th>SellerIsPriv</th>\n",
       "      <th>SellerListSrc</th>\n",
       "      <th>SellerName</th>\n",
       "      <th>SellerRating</th>\n",
       "      <th>SellerRevCnt</th>\n",
       "      <th>SellerState</th>\n",
       "      <th>SellerZip</th>\n",
       "      <th>VehCertified</th>\n",
       "      <th>VehColorExt</th>\n",
       "      <th>...</th>\n",
       "      <th>VehListdays</th>\n",
       "      <th>VehMake</th>\n",
       "      <th>VehMileage</th>\n",
       "      <th>VehModel</th>\n",
       "      <th>VehPriceLabel</th>\n",
       "      <th>VehSellerNotes</th>\n",
       "      <th>VehYear</th>\n",
       "      <th>Vehicle_Trim</th>\n",
       "      <th>Dealer_Listing_Price</th>\n",
       "      <th>NumOwners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warren</td>\n",
       "      <td>False</td>\n",
       "      <td>inventorycommandcenter</td>\n",
       "      <td>primemotorz</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32</td>\n",
       "      <td>MI</td>\n",
       "      <td>48091</td>\n",
       "      <td>False</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600069</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>39319</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>fairprice</td>\n",
       "      <td>None.</td>\n",
       "      <td>2015</td>\n",
       "      <td>High Altitude</td>\n",
       "      <td>30990.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fargo</td>\n",
       "      <td>False</td>\n",
       "      <td>cadillaccertifiedprogram</td>\n",
       "      <td>gatewaychevroletcadillac</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1456</td>\n",
       "      <td>ND</td>\n",
       "      <td>58103</td>\n",
       "      <td>True</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>2.920127</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>30352</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Come take a look at our great pre-owned invent...</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34860.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>waukesha</td>\n",
       "      <td>False</td>\n",
       "      <td>jeepcertifiedprogram</td>\n",
       "      <td>wildechryslerjeepdodgeramampsubaru</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1405</td>\n",
       "      <td>WI</td>\n",
       "      <td>53186</td>\n",
       "      <td>True</td>\n",
       "      <td>brilliantblackcrystalpearlcoat</td>\n",
       "      <td>...</td>\n",
       "      <td>28.107014</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>38957</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Backed by a rigorous 125-point inspection by f...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Laredo</td>\n",
       "      <td>23249.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wentzville</td>\n",
       "      <td>False</td>\n",
       "      <td>inventorycommandcenter</td>\n",
       "      <td>centurydodgechryslerjeepram</td>\n",
       "      <td>4.4</td>\n",
       "      <td>21</td>\n",
       "      <td>MO</td>\n",
       "      <td>63385</td>\n",
       "      <td>False</td>\n",
       "      <td>diamondblackcrystalpearlcoat</td>\n",
       "      <td>...</td>\n",
       "      <td>59.816875</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>20404</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Drop by to see us and you will quickly see how...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Limited</td>\n",
       "      <td>31977.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fayetteville</td>\n",
       "      <td>False</td>\n",
       "      <td>homenetautomotive</td>\n",
       "      <td>superiorbuickgmcoffayetteville</td>\n",
       "      <td>3.7</td>\n",
       "      <td>74</td>\n",
       "      <td>AR</td>\n",
       "      <td>72703</td>\n",
       "      <td>False</td>\n",
       "      <td>radiantsilvermetallic</td>\n",
       "      <td>...</td>\n",
       "      <td>98.665301</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>19788</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Luxury, Exterior Parking Camera Rear, Front Du...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>33495.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>dearborn</td>\n",
       "      <td>True</td>\n",
       "      <td>sellityourself</td>\n",
       "      <td>abe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>MI</td>\n",
       "      <td>48126</td>\n",
       "      <td>False</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>29.781968</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>49000</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>****ALL BLACK EDITION****You are viewing a bea...</td>\n",
       "      <td>2015</td>\n",
       "      <td>High Altitude</td>\n",
       "      <td>18699.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6290</th>\n",
       "      <td>indianapolis</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>carmaxindianapolis</td>\n",
       "      <td>3.3</td>\n",
       "      <td>16</td>\n",
       "      <td>IN</td>\n",
       "      <td>46280</td>\n",
       "      <td>False</td>\n",
       "      <td>gray</td>\n",
       "      <td>...</td>\n",
       "      <td>4.840069</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>20039</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>fairprice</td>\n",
       "      <td>CarMax makes car buying easy and hassle-free. ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Limited</td>\n",
       "      <td>31998.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6291</th>\n",
       "      <td>dublin</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>cadillacofdublin</td>\n",
       "      <td>4.1</td>\n",
       "      <td>20</td>\n",
       "      <td>OH</td>\n",
       "      <td>43017</td>\n",
       "      <td>True</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>184.921991</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>16278</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Clean CARFAX. Certified. Black 2018 Cadillac X...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>35674.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>sandusky</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>fosterchevroletcadillac</td>\n",
       "      <td>4.9</td>\n",
       "      <td>278</td>\n",
       "      <td>OH</td>\n",
       "      <td>44870</td>\n",
       "      <td>False</td>\n",
       "      <td>black</td>\n",
       "      <td>...</td>\n",
       "      <td>73.868426</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>38146</td>\n",
       "      <td>xt5</td>\n",
       "      <td>greatdeal</td>\n",
       "      <td>Black 2017 Cadillac XT5 Luxury FWD 8-Speed Aut...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>31995.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>nashville</td>\n",
       "      <td>False</td>\n",
       "      <td>homenetautomotive</td>\n",
       "      <td>vroomonlinedealernationwidedelivery</td>\n",
       "      <td>3.8</td>\n",
       "      <td>727</td>\n",
       "      <td>TN</td>\n",
       "      <td>37207</td>\n",
       "      <td>False</td>\n",
       "      <td>diamondblackcrystalpearlcoat</td>\n",
       "      <td>...</td>\n",
       "      <td>20.678600</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>17806</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>With Vroom, you can buy your next car from the...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Trailhawk</td>\n",
       "      <td>36280.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6294 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SellerCity  SellerIsPriv             SellerListSrc  \\\n",
       "0           warren         False    inventorycommandcenter   \n",
       "1            fargo         False  cadillaccertifiedprogram   \n",
       "2         waukesha         False      jeepcertifiedprogram   \n",
       "3       wentzville         False    inventorycommandcenter   \n",
       "4     fayetteville         False         homenetautomotive   \n",
       "...            ...           ...                       ...   \n",
       "6289      dearborn          True            sellityourself   \n",
       "6290  indianapolis         False      digitalmotorworksdmi   \n",
       "6291        dublin         False      digitalmotorworksdmi   \n",
       "6292      sandusky         False      digitalmotorworksdmi   \n",
       "6293     nashville         False         homenetautomotive   \n",
       "\n",
       "                               SellerName  SellerRating  SellerRevCnt  \\\n",
       "0                             primemotorz           5.0            32   \n",
       "1                gatewaychevroletcadillac           4.8          1456   \n",
       "2      wildechryslerjeepdodgeramampsubaru           4.8          1405   \n",
       "3             centurydodgechryslerjeepram           4.4            21   \n",
       "4          superiorbuickgmcoffayetteville           3.7            74   \n",
       "...                                   ...           ...           ...   \n",
       "6289                                  abe           0.0             0   \n",
       "6290                   carmaxindianapolis           3.3            16   \n",
       "6291                     cadillacofdublin           4.1            20   \n",
       "6292              fosterchevroletcadillac           4.9           278   \n",
       "6293  vroomonlinedealernationwidedelivery           3.8           727   \n",
       "\n",
       "     SellerState  SellerZip  VehCertified                     VehColorExt  \\\n",
       "0             MI      48091         False                           white   \n",
       "1             ND      58103          True                           black   \n",
       "2             WI      53186          True  brilliantblackcrystalpearlcoat   \n",
       "3             MO      63385         False    diamondblackcrystalpearlcoat   \n",
       "4             AR      72703         False           radiantsilvermetallic   \n",
       "...          ...        ...           ...                             ...   \n",
       "6289          MI      48126         False                           black   \n",
       "6290          IN      46280         False                            gray   \n",
       "6291          OH      43017          True                           black   \n",
       "6292          OH      44870         False                           black   \n",
       "6293          TN      37207         False    diamondblackcrystalpearlcoat   \n",
       "\n",
       "      ... VehListdays   VehMake VehMileage       VehModel VehPriceLabel  \\\n",
       "0     ...    8.600069      Jeep      39319  grandcherokee     fairprice   \n",
       "1     ...    2.920127  Cadillac      30352            xt5      gooddeal   \n",
       "2     ...   28.107014      Jeep      38957  grandcherokee      gooddeal   \n",
       "3     ...   59.816875      Jeep      20404  grandcherokee      gooddeal   \n",
       "4     ...   98.665301  Cadillac      19788            xt5      gooddeal   \n",
       "...   ...         ...       ...        ...            ...           ...   \n",
       "6289  ...   29.781968      Jeep      49000  grandcherokee           NaN   \n",
       "6290  ...    4.840069      Jeep      20039  grandcherokee     fairprice   \n",
       "6291  ...  184.921991  Cadillac      16278            xt5      gooddeal   \n",
       "6292  ...   73.868426  Cadillac      38146            xt5     greatdeal   \n",
       "6293  ...   20.678600      Jeep      17806  grandcherokee      gooddeal   \n",
       "\n",
       "                                         VehSellerNotes  VehYear  \\\n",
       "0                                                 None.     2015   \n",
       "1     Come take a look at our great pre-owned invent...     2017   \n",
       "2     Backed by a rigorous 125-point inspection by f...     2015   \n",
       "3     Drop by to see us and you will quickly see how...     2018   \n",
       "4     Luxury, Exterior Parking Camera Rear, Front Du...     2018   \n",
       "...                                                 ...      ...   \n",
       "6289  ****ALL BLACK EDITION****You are viewing a bea...     2015   \n",
       "6290  CarMax makes car buying easy and hassle-free. ...     2015   \n",
       "6291  Clean CARFAX. Certified. Black 2018 Cadillac X...     2018   \n",
       "6292  Black 2017 Cadillac XT5 Luxury FWD 8-Speed Aut...     2017   \n",
       "6293  With Vroom, you can buy your next car from the...     2018   \n",
       "\n",
       "       Vehicle_Trim  Dealer_Listing_Price NumOwners  \n",
       "0     High Altitude               30990.0       1.0  \n",
       "1               NaN               34860.0       1.0  \n",
       "2            Laredo               23249.0       1.0  \n",
       "3           Limited               31977.0       1.0  \n",
       "4            Luxury               33495.0       1.0  \n",
       "...             ...                   ...       ...  \n",
       "6289  High Altitude               18699.0       NaN  \n",
       "6290        Limited               31998.0       1.0  \n",
       "6291         Luxury               35674.0       0.0  \n",
       "6292         Luxury               31995.0       2.0  \n",
       "6293      Trailhawk               36280.0       1.0  \n",
       "\n",
       "[6294 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SellerCity               object\n",
       "SellerIsPriv               bool\n",
       "SellerListSrc            object\n",
       "SellerName               object\n",
       "SellerRating            float64\n",
       "SellerRevCnt              int64\n",
       "SellerState              object\n",
       "SellerZip                 int64\n",
       "VehCertified               bool\n",
       "VehColorExt              object\n",
       "VehColorInt              object\n",
       "VehDriveTrain            object\n",
       "VehEngine                object\n",
       "VehFeats                 object\n",
       "VehFuel                  object\n",
       "VehHistory               object\n",
       "VehListdays             float64\n",
       "VehMake                  object\n",
       "VehMileage                int64\n",
       "VehModel                 object\n",
       "VehPriceLabel            object\n",
       "VehSellerNotes           object\n",
       "VehYear                   int64\n",
       "Vehicle_Trim             object\n",
       "Dealer_Listing_Price    float64\n",
       "NumOwners               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SellerCity               object\n",
       "SellerIsPriv               bool\n",
       "SellerListSrc            object\n",
       "SellerName               object\n",
       "SellerRating            float64\n",
       "SellerRevCnt              int64\n",
       "SellerState              object\n",
       "SellerZip                 int64\n",
       "VehCertified               bool\n",
       "VehColorExt              object\n",
       "VehColorInt              object\n",
       "VehDriveTrain            object\n",
       "VehEngine                object\n",
       "VehFeats                 object\n",
       "VehFuel                  object\n",
       "VehHistory               object\n",
       "VehListdays             float64\n",
       "VehMake                  object\n",
       "VehMileage                int64\n",
       "VehModel                 object\n",
       "VehPriceLabel            object\n",
       "VehSellerNotes           object\n",
       "VehYear                   int64\n",
       "Vehicle_Trim             object\n",
       "Dealer_Listing_Price    float64\n",
       "NumOwners               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ListingID</th>\n",
       "      <th>SellerCity</th>\n",
       "      <th>SellerIsPriv</th>\n",
       "      <th>SellerListSrc</th>\n",
       "      <th>SellerName</th>\n",
       "      <th>SellerRating</th>\n",
       "      <th>SellerRevCnt</th>\n",
       "      <th>SellerState</th>\n",
       "      <th>SellerZip</th>\n",
       "      <th>VehCertified</th>\n",
       "      <th>...</th>\n",
       "      <th>VehFuel</th>\n",
       "      <th>VehHistory</th>\n",
       "      <th>VehListdays</th>\n",
       "      <th>VehMake</th>\n",
       "      <th>VehMileage</th>\n",
       "      <th>VehModel</th>\n",
       "      <th>VehPriceLabel</th>\n",
       "      <th>VehSellerNotes</th>\n",
       "      <th>VehYear</th>\n",
       "      <th>NumOwners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8622015</td>\n",
       "      <td>seneca</td>\n",
       "      <td>False</td>\n",
       "      <td>homenetautomotive</td>\n",
       "      <td>lakekeoweechryslerdodgejeepram</td>\n",
       "      <td>2.5</td>\n",
       "      <td>59</td>\n",
       "      <td>SC</td>\n",
       "      <td>29678</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['non-personal use reported', 'buyback protect...</td>\n",
       "      <td>143.991262</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>13625.0</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Thank you for visiting another one of Lake Keo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8625693</td>\n",
       "      <td>bedford</td>\n",
       "      <td>False</td>\n",
       "      <td>inventorycommandcenter</td>\n",
       "      <td>northcoastautomall</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2116</td>\n",
       "      <td>OH</td>\n",
       "      <td>44146</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['accident(s) reported', 'non-personal use rep...</td>\n",
       "      <td>138.770486</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>42553.0</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>This 2017 Jeep Grand Cherokee 4dr Limited 4x4 ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8625750</td>\n",
       "      <td>webster</td>\n",
       "      <td>False</td>\n",
       "      <td>jeepcertifiedprogram</td>\n",
       "      <td>marinachryslerdodgejeepmitsubishiram</td>\n",
       "      <td>3.9</td>\n",
       "      <td>46</td>\n",
       "      <td>NY</td>\n",
       "      <td>14580</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>E85 Flex Fuel</td>\n",
       "      <td>['buyback protection eligible']</td>\n",
       "      <td>31.951088</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>48951.0</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Certified. Brilliant Black Crystal Pearlcoat 2...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8626885</td>\n",
       "      <td>louisville</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>oxmoorfordlincoln</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1075</td>\n",
       "      <td>KY</td>\n",
       "      <td>40222</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['buyback protection eligible']</td>\n",
       "      <td>5.950127</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>44179.0</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>2015 Jeep Grand Cherokee ***THIS VEHICLE IS AT...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8627430</td>\n",
       "      <td>palmyra</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>fckerbeckampsons</td>\n",
       "      <td>4.6</td>\n",
       "      <td>162</td>\n",
       "      <td>NJ</td>\n",
       "      <td>8065</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['non-personal use reported', 'buyback protect...</td>\n",
       "      <td>24.672986</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>22269.0</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>AWD, CarFax One Owner! Navigation, Back-up Cam...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9992442</td>\n",
       "      <td>forestpark</td>\n",
       "      <td>False</td>\n",
       "      <td>homenetautomotive</td>\n",
       "      <td>curriechevy</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1081</td>\n",
       "      <td>IL</td>\n",
       "      <td>60130</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['buyback protection eligible']</td>\n",
       "      <td>18.091597</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>24744.0</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>granite crystal metallic clearcoat 2017 Jeep G...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9993562</td>\n",
       "      <td>tampa</td>\n",
       "      <td>False</td>\n",
       "      <td>inventorycommandcenter</td>\n",
       "      <td>tampamitsubishi</td>\n",
       "      <td>4.0</td>\n",
       "      <td>240</td>\n",
       "      <td>FL</td>\n",
       "      <td>33614</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['non-personal use reported', 'buyback protect...</td>\n",
       "      <td>167.799676</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>5699.0</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Tampa Mitsubishi is proud to offer this attrac...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>9994646</td>\n",
       "      <td>tampa</td>\n",
       "      <td>False</td>\n",
       "      <td>homenetautomotive</td>\n",
       "      <td>fermanacura</td>\n",
       "      <td>5.0</td>\n",
       "      <td>134</td>\n",
       "      <td>FL</td>\n",
       "      <td>33612</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['accident(s) reported', 'non-personal use rep...</td>\n",
       "      <td>46.215625</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>17985.0</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>*BRAND NEW* *2018* *CADILLAC* *XT5** LOADED WI...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9997199</td>\n",
       "      <td>hamburg</td>\n",
       "      <td>False</td>\n",
       "      <td>inventorycommandcenter</td>\n",
       "      <td>townechryslerdodgejeepraminc</td>\n",
       "      <td>3.8</td>\n",
       "      <td>7</td>\n",
       "      <td>NY</td>\n",
       "      <td>14075</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>['buyback protection eligible']</td>\n",
       "      <td>14.907535</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>27.0</td>\n",
       "      <td>grandcherokee</td>\n",
       "      <td>fairprice</td>\n",
       "      <td>Thousand?s of Vehicles, Positively Different E...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9999562</td>\n",
       "      <td>rocksprings</td>\n",
       "      <td>False</td>\n",
       "      <td>digitalmotorworksdmi</td>\n",
       "      <td>whislerchevrolet</td>\n",
       "      <td>4.9</td>\n",
       "      <td>28</td>\n",
       "      <td>WY</td>\n",
       "      <td>82901</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>['non-personal use reported', 'buyback protect...</td>\n",
       "      <td>144.833935</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>23435.0</td>\n",
       "      <td>xt5</td>\n",
       "      <td>gooddeal</td>\n",
       "      <td>Features: 2018 Cadillac XT5 Luxury Red AWD 3.6...</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ListingID   SellerCity  SellerIsPriv           SellerListSrc  \\\n",
       "0      8622015       seneca         False       homenetautomotive   \n",
       "1      8625693      bedford         False  inventorycommandcenter   \n",
       "2      8625750      webster         False    jeepcertifiedprogram   \n",
       "3      8626885   louisville         False    digitalmotorworksdmi   \n",
       "4      8627430      palmyra         False    digitalmotorworksdmi   \n",
       "..         ...          ...           ...                     ...   \n",
       "995    9992442   forestpark         False       homenetautomotive   \n",
       "996    9993562        tampa         False  inventorycommandcenter   \n",
       "997    9994646        tampa         False       homenetautomotive   \n",
       "998    9997199      hamburg         False  inventorycommandcenter   \n",
       "999    9999562  rocksprings         False    digitalmotorworksdmi   \n",
       "\n",
       "                               SellerName  SellerRating  SellerRevCnt  \\\n",
       "0          lakekeoweechryslerdodgejeepram           2.5            59   \n",
       "1                      northcoastautomall           4.7          2116   \n",
       "2    marinachryslerdodgejeepmitsubishiram           3.9            46   \n",
       "3                       oxmoorfordlincoln           4.5          1075   \n",
       "4                        fckerbeckampsons           4.6           162   \n",
       "..                                    ...           ...           ...   \n",
       "995                           curriechevy           4.8          1081   \n",
       "996                       tampamitsubishi           4.0           240   \n",
       "997                           fermanacura           5.0           134   \n",
       "998          townechryslerdodgejeepraminc           3.8             7   \n",
       "999                      whislerchevrolet           4.9            28   \n",
       "\n",
       "    SellerState  SellerZip  VehCertified  ...        VehFuel  \\\n",
       "0            SC      29678         False  ...       Gasoline   \n",
       "1            OH      44146         False  ...       Gasoline   \n",
       "2            NY      14580          True  ...  E85 Flex Fuel   \n",
       "3            KY      40222         False  ...       Gasoline   \n",
       "4            NJ       8065         False  ...       Gasoline   \n",
       "..          ...        ...           ...  ...            ...   \n",
       "995          IL      60130         False  ...       Gasoline   \n",
       "996          FL      33614         False  ...       Gasoline   \n",
       "997          FL      33612         False  ...       Gasoline   \n",
       "998          NY      14075         False  ...         Diesel   \n",
       "999          WY      82901         False  ...       Gasoline   \n",
       "\n",
       "                                            VehHistory VehListdays   VehMake  \\\n",
       "0    ['non-personal use reported', 'buyback protect...  143.991262  Cadillac   \n",
       "1    ['accident(s) reported', 'non-personal use rep...  138.770486      Jeep   \n",
       "2                      ['buyback protection eligible']   31.951088      Jeep   \n",
       "3                      ['buyback protection eligible']    5.950127      Jeep   \n",
       "4    ['non-personal use reported', 'buyback protect...   24.672986  Cadillac   \n",
       "..                                                 ...         ...       ...   \n",
       "995                    ['buyback protection eligible']   18.091597      Jeep   \n",
       "996  ['non-personal use reported', 'buyback protect...  167.799676  Cadillac   \n",
       "997  ['accident(s) reported', 'non-personal use rep...   46.215625  Cadillac   \n",
       "998                    ['buyback protection eligible']   14.907535      Jeep   \n",
       "999  ['non-personal use reported', 'buyback protect...  144.833935  Cadillac   \n",
       "\n",
       "    VehMileage       VehModel VehPriceLabel  \\\n",
       "0      13625.0            xt5      gooddeal   \n",
       "1      42553.0  grandcherokee      gooddeal   \n",
       "2      48951.0  grandcherokee      gooddeal   \n",
       "3      44179.0  grandcherokee      gooddeal   \n",
       "4      22269.0            xt5      gooddeal   \n",
       "..         ...            ...           ...   \n",
       "995    24744.0  grandcherokee      gooddeal   \n",
       "996     5699.0            xt5      gooddeal   \n",
       "997    17985.0            xt5      gooddeal   \n",
       "998       27.0  grandcherokee     fairprice   \n",
       "999    23435.0            xt5      gooddeal   \n",
       "\n",
       "                                        VehSellerNotes VehYear  NumOwners  \n",
       "0    Thank you for visiting another one of Lake Keo...    2018        1.0  \n",
       "1    This 2017 Jeep Grand Cherokee 4dr Limited 4x4 ...    2017        1.0  \n",
       "2    Certified. Brilliant Black Crystal Pearlcoat 2...    2015        1.0  \n",
       "3    2015 Jeep Grand Cherokee ***THIS VEHICLE IS AT...    2015        1.0  \n",
       "4    AWD, CarFax One Owner! Navigation, Back-up Cam...    2018        1.0  \n",
       "..                                                 ...     ...        ...  \n",
       "995  granite crystal metallic clearcoat 2017 Jeep G...    2017        1.0  \n",
       "996  Tampa Mitsubishi is proud to offer this attrac...    2017        1.0  \n",
       "997  *BRAND NEW* *2018* *CADILLAC* *XT5** LOADED WI...    2018        1.0  \n",
       "998  Thousand?s of Vehicles, Positively Different E...    2018        1.0  \n",
       "999  Features: 2018 Cadillac XT5 Luxury Red AWD 3.6...    2018        1.0  \n",
       "\n",
       "[1000 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nulls_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nulls = data[[\"Dealer_Listing_Price\", \"Vehicle_Trim\"]]\n",
    "# y_train_null_id = y_train_nulls[data.isnull().any(axis=1)].index\n",
    "\n",
    "\n",
    "X_train_nulls_raw = data.drop(\n",
    "    [\"Dealer_Listing_Price\", \"Vehicle_Trim\"], axis=1\n",
    ")  # nulls to indicate nulls present and raw to indicate ft are not numerically encoded yet\n",
    "y_test_id_nulls_raw = X_test_nulls_raw[\"ListingID\"]  # save for later\n",
    "X_test_nulls_raw = X_test_nulls_raw.drop([\"ListingID\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note some features that only have two options and are binarized. Recall that OrdinalEncoder is suitable for categorical variables with a meaningful order. OneHotEncoder is suitable for categorical variables without a natural specific order and can be used to binarize. Seller rating is tricky to deal with as it's a numeric categorical feature as ratings are still an ordinal measure (what does 3.5 rating - 2.0 vs 1.0 + 5.0 rating even mean??) Before we tend to these pipelines simplify the codification of ext/int colors as there is a lot of repetition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white' 'black' 'silver' 'blue' 'red' 'granite' 'gray' 'brown' 'steel'\n",
      " 'charcoal' 'velvet' 'green' 'bronze' 'rhino' nan 'ivory' 'amethyst'\n",
      " 'sangria' 'maroon' 'cashmere' 'certifiedhemilthrpanoroofnav' 'purple'\n",
      " 'certifiedlthrroofnavcamera' 'burgundy' 'auburn' 'gold' 'grey' 'pearl'\n",
      " 'diamond' 'other' 'midnightsky' 'beige' 'tan' 'notspecified'\n",
      " 'certifiedlthrpanoroofnavcamera' 'platinum' 'pewter' 'mocha'\n",
      " 'shadowmetallic' 'gy' 'certifiedlthrpanoroofhotcoldseats' 'pink' 'billet'\n",
      " 'certifiedroofcamerahtdseats' 'beigh' 'midnightskymetallic' 'unspecified'\n",
      " 'brightsil'] 48\n",
      "['black' 'blue' 'granite' 'silver' 'white' nan 'red' 'beige' 'amethyst'\n",
      " 'gray' 'ivory' 'billet' 'rhino' 'deepcherry' 'sangria' 'bronze' 'green'\n",
      " 'velvet' 'steel' 'cashmere' 'brown' 'auburn' 'grey' 'midnightsky'\n",
      " 'burgundy' 'shadowmetallic' 'tan' 'notspecified' 'diamond' 'undetermined'\n",
      " 'mocha'] 31\n"
     ]
    }
   ],
   "source": [
    "# Words to match and replace with the corresponding color name\n",
    "colors_to_replace = [\n",
    "    \"blue\",\n",
    "    \"red\",\n",
    "    \"black\",\n",
    "    \"silver\",\n",
    "    \"white\",\n",
    "    \"cashmere\",\n",
    "    \"steel\",\n",
    "    \"granite\",\n",
    "    \"ivory\",\n",
    "    \"amethyst\",\n",
    "    \"green\",\n",
    "    \"gray\",\n",
    "    \"brown\",\n",
    "    \"bronze\",\n",
    "    \"auburn\",\n",
    "    \"sangria\",\n",
    "    \"mocha\",\n",
    "    \"rhino\",\n",
    "]\n",
    "\n",
    "# Constructing a regular expression pattern to match any portion containing the specified colors\n",
    "pattern = \"|\".join(colors_to_replace)\n",
    "\n",
    "# Replace any portion of the string containing 'blue', 'red', or 'black' etc with only the corresponding color name\n",
    "# The r'\\1' in the value parameter is a backreference to the matched portion, so it replaces the entire string with only the color name.\n",
    "X_train_nulls_raw[\"VehColorExt\"] = X_train_nulls_raw[\"VehColorExt\"].replace(\n",
    "    to_replace=f\".*({pattern}).*\", value=r\"\\1\", regex=True\n",
    ")\n",
    "X_test_nulls_raw[\"VehColorExt\"] = X_test_nulls_raw[\"VehColorExt\"].replace(\n",
    "    to_replace=f\".*({pattern}).*\", value=r\"\\1\", regex=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    X_train_nulls_raw[\"VehColorExt\"].unique(),\n",
    "    len(X_train_nulls_raw[\"VehColorExt\"].unique()),\n",
    ")\n",
    "print(\n",
    "    X_test_nulls_raw[\"VehColorExt\"].unique(),\n",
    "    len(X_test_nulls_raw[\"VehColorExt\"].unique()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white' 'black' 'silver' 'blue' 'red' 'granite' 'gray' 'brown' 'steel'\n",
      " 'charcoal' 'velvet' 'green' 'bronze' 'rhino' nan 'ivory' 'amethyst'\n",
      " 'sangria' 'maroon' 'cashmere' 'certifiedhemilthrpanoroofnav' 'purple'\n",
      " 'certifiedlthrroofnavcamera' 'burgundy' 'auburn' 'gold' 'grey' 'pearl'\n",
      " 'diamond' 'other' 'midnightsky' 'beige' 'tan' 'notspecified'\n",
      " 'certifiedlthrpanoroofnavcamera' 'platinum' 'pewter' 'mocha'\n",
      " 'shadowmetallic' 'gy' 'certifiedlthrpanoroofhotcoldseats' 'pink' 'billet'\n",
      " 'certifiedroofcamerahtdseats' 'beigh' 'midnightskymetallic' 'unspecified'\n",
      " 'brightsil'] 38\n",
      "['black' 'blue' 'granite' 'silver' 'white' nan 'red' 'beige' 'amethyst'\n",
      " 'gray' 'ivory' 'billet' 'rhino' 'deepcherry' 'sangria' 'bronze' 'green'\n",
      " 'velvet' 'steel' 'cashmere' 'brown' 'auburn' 'grey' 'midnightsky'\n",
      " 'burgundy' 'shadowmetallic' 'tan' 'notspecified' 'diamond' 'undetermined'\n",
      " 'mocha'] 22\n"
     ]
    }
   ],
   "source": [
    "X_train_nulls_raw[\"VehColorInt\"] = X_train_nulls_raw[\"VehColorInt\"].replace(\n",
    "    to_replace=f\".*({pattern}).*\", value=r\"\\1\", regex=True\n",
    ")\n",
    "X_test_nulls_raw[\"VehColorInt\"] = X_test_nulls_raw[\"VehColorInt\"].replace(\n",
    "    to_replace=f\".*({pattern}).*\", value=r\"\\1\", regex=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    X_train_nulls_raw[\"VehColorExt\"].unique(),\n",
    "    len(X_train_nulls_raw[\"VehColorInt\"].unique()),\n",
    ")\n",
    "print(\n",
    "    X_test_nulls_raw[\"VehColorExt\"].unique(),\n",
    "    len(X_test_nulls_raw[\"VehColorInt\"].unique()),\n",
    ")\n",
    "\n",
    "# Careful not to overreach and drop colors you do not see in the test set as it's meant to be beyond our purview. This was simply to cut down on complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['SellerCity', 'SellerIsPriv', 'SellerListSrc', 'SellerName',\n",
       "        'SellerRating', 'SellerRevCnt', 'SellerState', 'SellerZip',\n",
       "        'VehCertified', 'VehColorExt', 'VehColorInt', 'VehDriveTrain',\n",
       "        'VehEngine', 'VehFeats', 'VehFuel', 'VehHistory', 'VehListdays',\n",
       "        'VehMake', 'VehMileage', 'VehModel', 'VehPriceLabel', 'VehSellerNotes',\n",
       "        'VehYear', 'NumOwners'],\n",
       "       dtype='object'),\n",
       " (6294, 24),\n",
       " (1000, 24))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nulls_raw.columns, X_train_nulls_raw.shape, X_test_nulls_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focus on the text columns and use Word2Vec to study them. I left notes in spark.py if you want more details on rationale (doing it on this end as converting pyspark Vector type is hassle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   None.\n",
       "1       Come take a look at our great pre-owned invent...\n",
       "2       Backed by a rigorous 125-point inspection by f...\n",
       "3       Drop by to see us and you will quickly see how...\n",
       "4       Luxury, Exterior Parking Camera Rear, Front Du...\n",
       "                              ...                        \n",
       "6289    ****ALL BLACK EDITION****You are viewing a bea...\n",
       "6290    CarMax makes car buying easy and hassle-free. ...\n",
       "6291    Clean CARFAX. Certified. Black 2018 Cadillac X...\n",
       "6292    Black 2017 Cadillac XT5 Luxury FWD 8-Speed Aut...\n",
       "6293    With Vroom, you can buy your next car from the...\n",
       "Name: VehSellerNotes, Length: 6294, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nulls_raw[\"VehSellerNotes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gbert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "# https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def tagDoc(df, col, tokens_only=False):\n",
    "    # Using a custom tokenization to remove stop words\n",
    "    # and other punctuation via regex\n",
    "    df[col] = df[col].fillna(\"none\")\n",
    "    df[col] = df[col].replace(\"\", \"none\")\n",
    "\n",
    "    df[col] = df[col].apply(\n",
    "        lambda text: [\n",
    "            re.sub(r\"[^a-zA-Z0-9]\", \"\", token.lower())\n",
    "            for token in word_tokenize(text)\n",
    "            if token.lower() not in stop_words\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # removing any resulting blank space entries like in  ['arsenic', '', 'cat'] --> ['arsenic', 'cat']\n",
    "    df[col] = df[col].apply(lambda text: \" \".join(text).split())\n",
    "\n",
    "    # now each entry (document) of the col has words in our custom tokenized format:\n",
    "    \"\"\"\n",
    "    DF now looks something like this (train shown here):\n",
    "    0                                               [none]\n",
    "    1    [come, take, look, great, preowned, inventory,...\n",
    "    2    [backed, rigorous, 125point, inspection, facto...\n",
    "    3    [drop, see, us, quickly, see, century, x27, sp...\n",
    "    4    [luxury, exterior, parking, camera, rear, fron...\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \"\"\"\n",
    "\n",
    "    for i, line in enumerate(df[col]):\n",
    "        # print(line)\n",
    "        if tokens_only:\n",
    "            yield line\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            # Example: TaggedDocument(words=['come', 'take', 'look', 'great', 'preowned', 'inventory', 'wwwgatewayfargocom'], tags=[1])\n",
    "            yield gensim.models.doc2vec.TaggedDocument(line, [i])\n",
    "\n",
    "\n",
    "def trainedWordModel(train_corpus, vector_size=100, min_count=3, epochs=40):\n",
    "    model = gensim.models.doc2vec.Doc2Vec(\n",
    "        vector_size=vector_size, min_count=min_count, epochs=epochs\n",
    "    )\n",
    "\n",
    "    # Essentially, the vocabulary is a list (accessible via model.wv.index_to_key)\n",
    "    # of all of the unique words extracted from the training corpus.\n",
    "    model.build_vocab(train_corpus)\n",
    "\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # Now, we can use the trained model to infer a vector for any piece of text by passing a list of words to the model.infer_vector function\n",
    "    return model\n",
    "\n",
    "\n",
    "def embed(model, df, col):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            df.at[index, col] = model.infer_vector(row[col])\n",
    "        except:\n",
    "            df.at[index, col] = np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2Vectorize = [\"VehSellerNotes\", \"SellerName\", \"SellerCity\"]\n",
    "for col in str2Vectorize:\n",
    "    train_corpus = list(tagDoc(X_train_nulls_raw, col, tokens_only=False))\n",
    "    test_corpus = list(tagDoc(X_test_nulls_raw, col, tokens_only=True))\n",
    "    # print(list(train_corpus)[0:3])\n",
    "    model = trainedWordModel(train_corpus=train_corpus)\n",
    "    embed(model, X_train_nulls_raw, col)\n",
    "    embed(model, X_test_nulls_raw, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we will stop here with NLP, one would consider using dimensionality reduction techniques to gauge the explainability of the encoding. Perhaps changes to vector lengths, min word counts in a corpus, etc for better discriminatory power. Formatting string data in these dense vectors can be used in ranking and recommender systems or in sentiment analysis, for example. To cut down on some complexity (perhaps with dire effect -- A/B test may show impacts to metrics on this choice), we just take the L2 norm of the vector as a feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedNormals(df, col):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            df.at[index, col] = np.linalg.norm(row[col].astype(float))\n",
    "        except:\n",
    "            print(f\"WARNING OF DOOM FOR {col} AT INDEX {index}\")\n",
    "            df.at[index, col] = 0.0\n",
    "\n",
    "\n",
    "for col in str2Vectorize:\n",
    "    embedNormals(X_train_nulls_raw, col)\n",
    "    embedNormals(X_test_nulls_raw, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed it is best to check correlation measures for catgorical and numerical features. We shall use Cramer's V Test for categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import itertools\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V#cite_note-Ref_a-2\n",
    "\n",
    "\n",
    "def calculate_cramers_v(tabs):\n",
    "    \"\"\"\n",
    "    Cramér's V varies from 0 (corresponding to no association between the variables)\n",
    "    to 1 (complete association) and can reach 1 only when each variable is completely determined by the other.\n",
    "    It may be viewed as the association between two discrete variables as a percentage of their maximum possible variation.\n",
    "    \"\"\"\n",
    "    confusion_matrix = tabs\n",
    "    chi2, _, _, _ = chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "\n",
    "    # Adding checks to avoid division by zero\n",
    "    if min((k - 1), (r - 1)) == 0 or phi2 == 0:\n",
    "        return 0\n",
    "\n",
    "    # This is the bias correction: https://en.wikipedia.org/wiki/Cram%C3%A9r's_V\n",
    "    phi2 = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "\n",
    "    # Adding checks to avoid division by zero\n",
    "    if min((kcorr - 1), (rcorr - 1)) == 0:\n",
    "        return 0\n",
    "\n",
    "    return np.sqrt(phi2 / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "\n",
    "discrete_cat_variables = [\n",
    "    \"SellerIsPriv\",\n",
    "    \"SellerListSrc\",\n",
    "    \"SellerState\",\n",
    "    \"SellerZip\",\n",
    "    \"VehCertified\",\n",
    "    \"VehDriveTrain\",\n",
    "    \"VehEngine\",\n",
    "    \"VehFeats\",\n",
    "    \"VehFuel\",\n",
    "    \"VehMake\",\n",
    "    \"VehModel\",\n",
    "    \"VehPriceLabel\",\n",
    "    \"VehHistory\",\n",
    "    \"VehColorExt\",\n",
    "    \"VehColorInt\",\n",
    "]\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results_list = []\n",
    "\n",
    "pairs = list(itertools.combinations(discrete_cat_variables, 2))\n",
    "\n",
    "for x, y in pairs:\n",
    "    # creating confusion matrix data frame\n",
    "    tabs = pd.crosstab(X_train_nulls_raw[x], X_train_nulls_raw[y])\n",
    "    cV = calculate_cramers_v(tabs)\n",
    "\n",
    "    if cV >= 0.5:\n",
    "        results_list.append({\"Variable 1\": x, \"Variable 2\": y, \"Cramer's V\": cV})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "results_df = pd.DataFrame(results_list).sort_values(by=[\"Cramer's V\"], ascending=False)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make (Cadillac/Jeep) and model (grandcherokee/xt5) of a car show complete association which is a no brainer -- Jeep Grand Chereokee or Cadillac XT5 are the models in train set! We drop the model feature in our encoding pipeline.\n",
    "\n",
    "Listing if from a private seller (T/F) and seller listing source (8 options) identifier also show complete determination: it may come as no suprise as \"certified program\" shows up on about half the unique entries of SellerListSrc. We favor the binary feature and drop the listing src as it would add to even more sparsity.\n",
    "\n",
    "Drive train and the vehicle make/model show complete determination. Jeeps and Caddys will may have unique drive trains or share certain configurations. We will keep make as model was already dropped. We shall instead drop drive train as it is nearly completely determined by which the vehicle make (Cadillac or Jeep) and suggests that AWD and 4WD/AWD could have been combined into one determination. Bluntly, knowing we have a FWD (or 2WD) option, we'd probably know which car make and model we're dealing with in this dataset.\n",
    "\n",
    "Lastly, we choose to drop VehEngine as it is nearly determined by make or model and has close association with fuel type. Note that even if features may have close determination, that does not necessarily mean either are important to a model. You would also have to look at feature importances separately: knowing A could determine B but show little influence into making predictions we set out to find.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at collinearity between numerical features. We first look at Spearman's over Pearson's coeff to better understand ordinal measures (if present) as the latter assumes underlying gaussians of each feature and linear relationships between the two. Spearman's coeff measures whether the relationship between variables is monotonic and doesn't strictly assume linearity. Since there may be some outliers in the dataset this test may be more telling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nulls_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "\n",
    "\n",
    "def calculate_spearman_corr(x, y):\n",
    "    corr_p = spearmanr(x, y)\n",
    "    return corr_p\n",
    "\n",
    "\n",
    "numerical_variables = [\n",
    "    \"SellerRevCnt\",\n",
    "    \"VehListdays\",\n",
    "    \"VehMileage\",\n",
    "    \"VehYear\",\n",
    "    \"NumOwners\",\n",
    "    \"VehSellerNotes\",\n",
    "    \"SellerName\",\n",
    "    \"SellerCity\",\n",
    "    \"SellerRating\",\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "pairs = list(itertools.combinations(numerical_variables, 2))\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "\n",
    "for x, y in pairs:\n",
    "    # Calculate Spearman's correlation for numerical features\n",
    "    corr_value, p_val = calculate_spearman_corr(\n",
    "        X_train_nulls_raw[x], X_train_nulls_raw[y]\n",
    "    )\n",
    "    # You can adjust the threshold as needed\n",
    "    if abs(corr_value) >= 0.1:\n",
    "        results_list.append(\n",
    "            {\n",
    "                \"Variable 1\": x,\n",
    "                \"Variable 2\": y,\n",
    "                \"+/-\": np.sign(corr_value),\n",
    "                \"|Spearman's Correlation|\": abs(corr_value),\n",
    "                \"P-Value\": p_val,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "results_df = pd.DataFrame(results_list).sort_values(\n",
    "    by=[\"|Spearman's Correlation|\"], ascending=False\n",
    ")\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value in statistics represents the probability of obtaining a result as extreme or more extreme than the observed result, assuming that the null hypothesis is true. In other words, it indicates how likely it is to observe the data if the null hypothesis is valid. A p-value near 0 in a statistical test typically indicates strong evidence against the null hypothesis. This suggests that the observed data is unlikely to have occurred by chance alone under the assumption that the null hypothesis is true. In this context, a small p-value indicates that there is ample evidence to reject the null hypothesis that there is no monotonic relationship between the two variables being compared. If there is a curvilinear but non-monotonic relationship, both Spearman’s and Pearson’s correlation will be close to zero.\n",
    "\n",
    "BUT while it may come as no surprise that mileage and year are strongly monotonic in this data (mileage usually decreases with increasing year -- it's likely that newer cars tend to have fewer mileage!) we will not drop any of these features as this test does not show causation; confounding variables or other underlying relationships can be playing into this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be convinced of any linear relationships in the underlying numerical data we show that below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant columns\n",
    "corrDF = X_train_nulls_raw[numerical_variables]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation = corrDF.corr()\n",
    "\n",
    "# Set a threshold for highlighting correlations\n",
    "threshold = 0.5\n",
    "\n",
    "# Create a mask to hide the upper triangle of the correlation matrix\n",
    "mask = np.triu(np.ones_like(correlation), k=1)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    correlation, annot=True, cmap=\"coolwarm\", mask=mask, fmt=\".2f\", vmin=-1, vmax=1\n",
    ")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VehYear-VehMileage Pearson coeff is slighly underestimated from its Spearman analogue which is not surpising given that a curvilinear nature confuses P's (so to speak), and it underestimates the relationship’s strength. Seller Name-Seller city show a stronger though moderate linear dependence and so does VehMileage-NumOwners. These could possibly be explained by the fact that seller cities tend to have a set group of sellers therein and the more owners a vehicle has had, the greater its mileage. No further discussion will be made in regards to this measure as the variables were never normally distributed and outliers may greatly be impacting the magnitude of this statistic; this was more illustrative of an underestimate we could have relied on had we not considered the assumptions that showed Spearman's to be more fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly turn to the target variable in our train and notice there are nans in both columns. We replace nans in the dealer list price column with in-group averages should the matching trim be available from the other column. So, if there is a missing list price I will get the average for the car trim that had those list prices, if any, and vice versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nulls[\"Vehicle_Trim\"].unique(), len(y_train_nulls[\"Vehicle_Trim\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priceLookup = y_train_nulls.groupby(\"Vehicle_Trim\")[\"Dealer_Listing_Price\"].median()\n",
    "pd.DataFrame(priceLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimLookup = y_train_nulls.groupby(\"Dealer_Listing_Price\")[\"Vehicle_Trim\"].agg(\n",
    "    pd.Series.mode\n",
    ")\n",
    "pd.DataFrame(trimLookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nulls[\"Vehicle_Trim\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mia = y_train_nulls[y_train_nulls.isnull().any(axis=1)]\n",
    "mia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_nulls.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultTrim = str(y_train_nulls.Vehicle_Trim.agg(pd.Series.mode)[0])\n",
    "defaultPrice = y_train_nulls.Dealer_Listing_Price.median()\n",
    "y_train = y_train_nulls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# https://stackoverflow.com/questions/68671852/best-way-to-iterate-through-elements-of-pandas-series\n",
    "\n",
    "\n",
    "# Function to find the first model trim within the given tolerance\n",
    "\n",
    "\n",
    "def find_model_trim(knownPrice):\n",
    "    # both trim and price are unknown (dreadful label) but have covariates\n",
    "    # will end up with both defaults or, depending on tolerance, variation\n",
    "    # with this default\n",
    "    if math.isnan(knownPrice):\n",
    "        return defaultTrim\n",
    "\n",
    "    tolerance = 0.04  # 4% tolerance\n",
    "\n",
    "    # Try to find comparable trim for that price within a tolerance\n",
    "    for trim, price in zip(priceLookup.index, priceLookup.to_numpy()):\n",
    "        if abs(knownPrice - price) / knownPrice <= tolerance:\n",
    "            return trim\n",
    "\n",
    "    return defaultTrim\n",
    "\n",
    "\n",
    "def find_model_price(knownTrim):\n",
    "    # both trim and price are unknown (dreadful label) but have covariates\n",
    "    # will end up with both defaults or, depending on tolerance, variation\n",
    "    # with this default\n",
    "\n",
    "    if knownTrim != knownTrim:\n",
    "        return defaultPrice\n",
    "\n",
    "    # Find the median price for that trim\n",
    "    priceFound = priceLookup.get(knownTrim, default=-1)\n",
    "    priceNew = priceFound + 500\n",
    "\n",
    "    if priceFound == -1:\n",
    "        # trim may not be in the corpus being used, attempt to\n",
    "        # find price for something slightly newer (more $)\n",
    "        upgrade = trimLookup.get(priceNew, default=defaultPrice)\n",
    "        priceFound = upgrade\n",
    "\n",
    "    # may be the case that multiple cars at the border price share that price\n",
    "    if isinstance(priceFound, (int, float)):\n",
    "        return priceFound  # Return the number directly\n",
    "    elif isinstance(priceFound, (list, np.ndarray)) and len(priceFound) > 0:\n",
    "        return priceFound[0]  # Return the first element of the array\n",
    "\n",
    "\n",
    "\n",
    "for index in y_train_nulls[y_train_nulls[\"Vehicle_Trim\"].isnull()].index:\n",
    "    y_train.loc[index, \"Vehicle_Trim\"] = find_model_trim(\n",
    "        y_train_nulls.loc[index, \"Dealer_Listing_Price\"]\n",
    "    )\n",
    "\n",
    "# Populate nulls in y with group avgs and their corr labels\n",
    "for index in y_train_nulls[y_train_nulls[\"Dealer_Listing_Price\"].isnull()].index:\n",
    "    y_train.loc[index, \"Dealer_Listing_Price\"] = find_model_price(\n",
    "        y_train_nulls.loc[index, \"Vehicle_Trim\"]\n",
    "    )\n",
    "\n",
    "print(len(y_train))\n",
    "nan_indices = y_train.isna().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.loc[~nan_indices]\n",
    "X_train_nulls_raw = X_train_nulls_raw.loc[~nan_indices] # no effect as correctly processed algo for in-group avgs\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.iloc[1, :], y_train_nulls.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['Vehicle_Trim'].value_counts(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['Vehicle_Trim'] = y_train['Vehicle_Trim'].replace(\n",
    "    {\n",
    "        'Limited 4x4': 'Limited',\n",
    "\t\t'Limited 75th Anniversary' : '75th Anniversary Edition',\n",
    "        'Limited 75th Anniversary Edition': '75th Anniversary Edition',\n",
    "\t\t'75th Anniversary' : '75th Anniversary Edition',\n",
    "\t\t'SRT Night' : 'SRT',\n",
    "\t\t'Upland' : 'FWD'\n",
    "\t\t\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['Vehicle_Trim'].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoding for y. We won't scale the numeric values of target here but on Pytorch to go back and forth with\n",
    "# with the true label and labels used for train. Since std scaler uses mean 0 and dev 1 its easy to do\n",
    "# for the OH encoding im not sure the scheme is used in pytorch like in sklearn so ill just\n",
    "# map back the endoded ft names to the true label names\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Vehicle_Trim\",\n",
    "]\n",
    "numeric_cols = [\n",
    "    \"Dealer_Listing_Price\",\n",
    "]\n",
    "\n",
    "# Create transformers\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create a final pipeline with the preprocessor and any subsequent steps (e.g., a model)\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # Add additional steps as needed, e.g., a machine learning model -- holding off as we want to do imputation study\n",
    "    ]\n",
    ")\n",
    "y = final_pipeline.fit_transform(\n",
    "    y_train\n",
    ")  # nothing is missing as nans were accounted for\n",
    "transformed_feature_names_y = final_pipeline.named_steps[\n",
    "    \"preprocessor\"\n",
    "].get_feature_names_out()\n",
    "\n",
    "\n",
    "# Create a DataFrame with a single column containing the feature names\n",
    "df = pd.DataFrame({\"Feature Names\": transformed_feature_names_y})\n",
    "# Save the DataFrame to a CSV file for pytorch later\n",
    "df.to_csv(\"transformed_feature_names_y.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nulls = pd.concat([X_train_nulls_raw, X_test_nulls_raw])\n",
    "X_nulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "\n",
    "# Define column subsets\n",
    "categorical_cols = [\n",
    "    \"SellerIsPriv\",\n",
    "    \"SellerState\",\n",
    "    \"SellerZip\",\n",
    "    \"VehCertified\",\n",
    "    \"VehColorExt\",\n",
    "    \"VehColorInt\",\n",
    "    \"VehFeats\",\n",
    "    \"VehFuel\",\n",
    "    \"VehHistory\",\n",
    "    \"VehMake\",\n",
    "]\n",
    "ordinal_cols = [\"SellerRating\", \"VehPriceLabel\"]\n",
    "numeric_cols = [\n",
    "    \"SellerRevCnt\",\n",
    "    \"VehListdays\",\n",
    "    \"VehMileage\",\n",
    "    \"VehYear\",\n",
    "    \"NumOwners\",\n",
    "    \"SellerName\",\n",
    "    \"SellerCity\",\n",
    "]\n",
    "\n",
    "# https://www.statology.org/dummy-variable-trap/\n",
    "# NOTE We want to avoid running into the \"dummy variable trap\" which will intro collinearity in features that have binary\n",
    "# output: SellerIsPriv, VehCertified, VehMake, VehModel (dropped later). Example: if jeep_VehModel is 0, then we know cadd_VehModel is 1 and make for linear dependence\n",
    "# For k different variables, use k-1 dummy variables. Since these features are mutually exclusive we have to drop_first for each of them separately\n",
    "# to the function\n",
    "get_dummies_cols = [\"SellerIsPriv\", \"VehCertified\", \"VehMake\"]\n",
    "\n",
    "\n",
    "# Custom transformer for get_dummies with drop_first=True\n",
    "def get_dummies_transformer(X):\n",
    "    return pd.get_dummies(X, columns=get_dummies_cols, drop_first=True)\n",
    "\n",
    "\n",
    "dropMulticollinearity = FunctionTransformer(\n",
    "    get_dummies_transformer, feature_names_out=\"one-to-one\"\n",
    ")\n",
    "\n",
    "# Create transformers\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"ordinal\",\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler(with_mean=True)),\n",
    "    ]  # https://stackoverflow.com/questions/52008548/python-running-into-x-test-y-test-fit-errors\n",
    ")\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat\",\n",
    "            categorical_transformer,\n",
    "            [col for col in categorical_cols if col not in get_dummies_cols],\n",
    "        ),\n",
    "        (\"get_dummies\", dropMulticollinearity, get_dummies_cols),  # custom step\n",
    "        (\"ord\", ordinal_transformer, ordinal_cols),\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")  # NOTE CRITICAL -- leave out the other features that showed multicollinearity w/other ft: VehModel, SellerListSrc, VehDriveTrain (had 401 null rows), VehEngine (had 361 null rows)\n",
    "# we also drop VehSellerNotes (243 nulls) as condensing so many words to magnitude (many of them spurious and hackneyed phrases aplomb) may generate a lot of noise in dataset\n",
    "\n",
    "# Create a final pipeline with the preprocessor and any subsequent steps (e.g., a model)\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # Add additional steps as needed, e.g., a machine learning model -- holding off as we want to do imputation study\n",
    "    ]\n",
    ")\n",
    "X = final_pipeline.fit_transform(X_nulls)\n",
    "transformed_feature_names_X = final_pipeline.named_steps[\n",
    "    \"preprocessor\"\n",
    "].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A expected (without using imputation within the pipeline) we should see some NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X)), np.any(np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nulls = X[: len(X_train_nulls_raw)]  # drop raw tag as now encoded\n",
    "X_test_nulls = X[len(X_train_nulls_raw) :]\n",
    "X_train_nulls.shape, X_test_nulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_train_nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have encoded the data let's take a look at the distributions (before the compression from encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distCol = [\n",
    "    \"VehListdays\",\n",
    "    \"VehMileage\",\n",
    "    \"NumOwners\",\n",
    "    \"SellerRating\",\n",
    "    \"SellerRevCnt\",\n",
    "]\n",
    "# Set up subplots -- NOTE: Dealer_Listing_Price is part of what we want to predict!\n",
    "fig, axes = plt.subplots(nrows=len(distCol), ncols=1, figsize=(8, 4 * len(distCol)))\n",
    "\n",
    "## Plot distribution and statistics for each column\n",
    "for i, column in enumerate(distCol):\n",
    "    # Plot distribution\n",
    "    sns.histplot(X_train_nulls_raw[column], kde=True, ax=axes[i])\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_value = X_train_nulls_raw[column].mean()\n",
    "    median_value = X_train_nulls_raw[column].median()\n",
    "    std_dev_value = X_train_nulls_raw[column].std()\n",
    "\n",
    "    # Plot major statistics below the distribution plot\n",
    "    axes[i].axvline(\n",
    "        mean_value,\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean ({mean_value:.2f})\",\n",
    "    )\n",
    "    axes[i].axvline(\n",
    "        median_value,\n",
    "        color=\"green\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Median ({median_value:.2f})\",\n",
    "    )\n",
    "    axes[i].axvline(\n",
    "        mean_value + std_dev_value,\n",
    "        color=\"orange\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Std Dev ({std_dev_value:.2f})\",\n",
    "    )\n",
    "    axes[i].axvline(\n",
    "        mean_value - std_dev_value, color=\"orange\", linestyle=\"dashed\", linewidth=2\n",
    "    )\n",
    "    axes[i].legend()\n",
    "\n",
    "    # Set x-axis limits to the minimum and maximum values of the distribution\n",
    "    axes[i].set_xlim(X_train_nulls_raw[column].min(), X_train_nulls_raw[column].max())\n",
    "\n",
    "    # Set plot labels and title\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].set_title(f\"Distribution of {column}\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to the variables\n",
    "X_train_log = np.log1p(X_train_nulls_raw[distCol])\n",
    "\n",
    "# Standardize the log-transformed variables using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_log_scaled = scaler.fit_transform(X_train_log)\n",
    "\n",
    "# Set up subplots with smaller font size\n",
    "fig, axes = plt.subplots(nrows=len(distCol), ncols=3, figsize=(24, 4 * len(distCol)))\n",
    "\n",
    "# Set smaller font size for tick labels and axis labels\n",
    "plt.rcParams.update({\"font.size\": 8})\n",
    "\n",
    "# Plot distribution and statistics for each original variable\n",
    "for i, column in enumerate(distCol):\n",
    "    # Plot distribution before log transformation\n",
    "    sns.histplot(X_train_nulls_raw[column], kde=True, ax=axes[i, 0])\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_value = X_train_nulls_raw[column].mean()\n",
    "    median_value = X_train_nulls_raw[column].median()\n",
    "    std_dev_value = X_train_nulls_raw[column].std()\n",
    "\n",
    "    # Plot major statistics before log transformation\n",
    "    axes[i, 0].axvline(\n",
    "        mean_value,\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean ({mean_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 0].axvline(\n",
    "        median_value,\n",
    "        color=\"green\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Median ({median_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 0].axvline(\n",
    "        mean_value + std_dev_value,\n",
    "        color=\"orange\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Std Dev ({std_dev_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 0].axvline(\n",
    "        mean_value - std_dev_value, color=\"orange\", linestyle=\"dashed\", linewidth=2\n",
    "    )\n",
    "    axes[i, 0].legend()\n",
    "\n",
    "    # Set x-axis limits to the minimum and maximum values of the distribution\n",
    "    axes[i, 0].set_xlim(\n",
    "        X_train_nulls_raw[column].min(), X_train_nulls_raw[column].max()\n",
    "    )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    axes[i, 0].set_xlabel(column)\n",
    "    axes[i, 0].set_ylabel(\"Frequency\")\n",
    "    axes[i, 0].set_title(f\"Distribution of {column} (Before Log Transformation)\")\n",
    "\n",
    "# Plot distribution and statistics for each log-transformed variable\n",
    "for i, column in enumerate(distCol):\n",
    "    # Plot distribution after log transformation\n",
    "    sns.histplot(X_train_log[column], kde=True, ax=axes[i, 1])\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_value = X_train_log[column].mean()\n",
    "    std_dev_value = X_train_log[column].std()\n",
    "\n",
    "    # Plot major statistics after log transformation\n",
    "    axes[i, 1].axvline(\n",
    "        mean_value,\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean ({mean_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 1].axvline(\n",
    "        mean_value + std_dev_value,\n",
    "        color=\"orange\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Std Dev ({std_dev_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 1].axvline(\n",
    "        mean_value - std_dev_value, color=\"orange\", linestyle=\"dashed\", linewidth=2\n",
    "    )\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "    # Set x-axis limits to the minimum and maximum values of the distribution\n",
    "    axes[i, 1].set_xlim(X_train_log[column].min(), X_train_log[column].max())\n",
    "\n",
    "    # Set plot labels and title\n",
    "    axes[i, 1].set_xlabel(f\"{column} (Log Transformed)\")\n",
    "    axes[i, 1].set_ylabel(\"Frequency\")\n",
    "    axes[i, 1].set_title(f\"Distribution of {column} (After Log Transformation)\")\n",
    "\n",
    "# Plot distribution and statistics for each log-transformed and standardized variable\n",
    "for i, column in enumerate(distCol):\n",
    "    # Handle NaN or Inf values after standard scaling (recall that log(0) = -inf!)\n",
    "    X_train_log_scaled_column = X_train_log_scaled[:, i]\n",
    "    X_train_log_scaled_column = X_train_log_scaled_column[\n",
    "        np.isfinite(X_train_log_scaled_column)\n",
    "    ]\n",
    "\n",
    "    # Plot distribution after log transformation and standard scaling\n",
    "    sns.histplot(X_train_log_scaled_column, kde=True, ax=axes[i, 2])\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_value = np.nanmean(X_train_log_scaled_column)\n",
    "    std_dev_value = np.nanstd(X_train_log_scaled_column)\n",
    "\n",
    "    # Plot major statistics after log transformation and standard scaling\n",
    "    axes[i, 2].axvline(\n",
    "        mean_value,\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean ({mean_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 2].axvline(\n",
    "        mean_value + std_dev_value,\n",
    "        color=\"orange\",\n",
    "        linestyle=\"dashed\",\n",
    "        linewidth=2,\n",
    "        label=f\"Std Dev ({std_dev_value:.2f})\",\n",
    "    )\n",
    "    axes[i, 2].axvline(\n",
    "        mean_value - std_dev_value, color=\"orange\", linestyle=\"dashed\", linewidth=2\n",
    "    )\n",
    "    axes[i, 2].legend()\n",
    "\n",
    "    # Set x-axis limits to the minimum and maximum values of the distribution\n",
    "    axes[i, 2].set_xlim(\n",
    "        X_train_log_scaled_column.min(), X_train_log_scaled_column.max()\n",
    "    )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    axes[i, 2].set_xlabel(f\"{column} (Log Transformed and Standardized)\")\n",
    "    axes[i, 2].set_ylabel(\"Frequency\")\n",
    "    axes[i, 2].set_title(\n",
    "        f\"Distribution of {column} (After Log Transformation and Standardization)\"\n",
    "    )\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OH SHOOT WE GOT HETEROSCEDASTIC (HETEREOGENEITY OF VARIANCE) FEATURES\n",
    "# https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va\n",
    "# https://stats.stackexchange.com/questions/23479/why-do-we-assume-that-the-error-is-normally-distributed\n",
    "# https://www.statology.org/heteroscedasticity-regression/\n",
    "# https://itfeature.com/introduction-reasons-and-consequences-of-heteroscedasticity\n",
    "\n",
    "Positive skewness often suggests that there are relatively large values in the \n",
    "right tail of the distribution as we are seeing here. These extreme values contribute to the higher \n",
    "variability or dispersion in the data sp by taking the log of the data compresses the scale, \n",
    "we are giving more weight to the smaller values and reducing the impact of the larger values. \n",
    "This helps stabilize the variance achieve greater symmetric.\n",
    "As we see here, it can make the distribution of a variable more approximately normal, which brings us to... \n",
    "\n",
    "Logistic regression assumes that the errors are normally distributed (CLT only applies to samples that are IID), and a log transformation\n",
    "can be effective in making the distribution of predictors more symmetric. \n",
    "If a feature is positively skewed, taking the logarithm can reduce the impact of extreme values, \n",
    "stabilize the variance, and make the distribution more symmetric, which aligns with\n",
    "the assumptions of logistic regression.\n",
    "\n",
    "SO why won't you add to the transforms? Great q: error checking, time, possible transformation of dependent variable costly,\n",
    "analysis of variance on getting outliers, weighted regression considerations (assign small weights to higher variance observations\n",
    " to decrease their respective squared residuals), but most importantly\n",
    "\n",
    " The log-odds transformation in logistic regression introduces a non-linearity that is not present in linear regression!\n",
    " Considering more robust methods will have me chatting with your Math Stats dept. Will try for validation set and early stopping\n",
    " in Pytorch instead. Robust regression: Huber loss is used because of this consideration for skewed price distribution.\n",
    " \n",
    " https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Set up subplots\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Plot distribution after Dropping Nulls\n",
    "\n",
    "sns.histplot(y_train[\"Dealer_Listing_Price\"], kde=True, ax=axes[0])\n",
    "\n",
    "\n",
    "# Calculate statistics after Dropping Nulls\n",
    "\n",
    "mean_value_after = y_train[\"Dealer_Listing_Price\"].mean()\n",
    "\n",
    "median_value_after = y_train[\"Dealer_Listing_Price\"].median()\n",
    "\n",
    "std_dev_value_after = y_train[\"Dealer_Listing_Price\"].std()\n",
    "\n",
    "\n",
    "# Plot major statistics after Dropping Nulls\n",
    "\n",
    "axes[0].axvline(\n",
    "    mean_value_after,\n",
    "    color=\"red\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean ({mean_value_after:.2f})\",\n",
    ")\n",
    "\n",
    "axes[0].axvline(\n",
    "    median_value_after,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median ({median_value_after:.2f})\",\n",
    ")\n",
    "\n",
    "axes[0].axvline(\n",
    "    mean_value_after + std_dev_value_after,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Std Dev ({std_dev_value_after:.2f})\",\n",
    ")\n",
    "\n",
    "axes[0].axvline(\n",
    "    mean_value_after - std_dev_value_after,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# Set x-axis limits to the minimum and maximum values of the distribution\n",
    "\n",
    "axes[0].set_xlim(\n",
    "    y_train[\"Dealer_Listing_Price\"].min(),\n",
    "    y_train[\"Dealer_Listing_Price\"].max(),\n",
    ")\n",
    "\n",
    "\n",
    "# Set plot labels and title for the first subplot\n",
    "\n",
    "axes[0].set_xlabel(\"Dealer_Listing_Price\")\n",
    "\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[0].set_title(\"Distribution after Dropping Nulls\")\n",
    "\n",
    "\n",
    "# Plot distribution before Dropping Nulls using y_train_nulls\n",
    "\n",
    "sns.histplot(y_train_nulls[\"Dealer_Listing_Price\"], kde=True, ax=axes[1])\n",
    "\n",
    "\n",
    "# Calculate statistics before Dropping Nulls\n",
    "\n",
    "mean_value_before = y_train_nulls[\"Dealer_Listing_Price\"].mean()\n",
    "\n",
    "median_value_before = y_train_nulls[\"Dealer_Listing_Price\"].median()\n",
    "\n",
    "std_dev_value_before = y_train_nulls[\"Dealer_Listing_Price\"].std()\n",
    "\n",
    "\n",
    "# Plot major statistics before Dropping Nulls\n",
    "\n",
    "axes[1].axvline(\n",
    "    mean_value_before,\n",
    "    color=\"red\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean ({mean_value_before:.2f})\",\n",
    ")\n",
    "\n",
    "axes[1].axvline(\n",
    "    median_value_before,\n",
    "    color=\"green\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median ({median_value_before:.2f})\",\n",
    ")\n",
    "\n",
    "axes[1].axvline(\n",
    "    mean_value_before + std_dev_value_before,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    "    label=f\"Std Dev ({std_dev_value_before:.2f})\",\n",
    ")\n",
    "\n",
    "axes[1].axvline(\n",
    "    mean_value_before - std_dev_value_before,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"dashed\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "axes[1].legend()\n",
    "\n",
    "\n",
    "# Set x-axis limits to the minimum and maximum values of the distribution\n",
    "\n",
    "axes[1].set_xlim(\n",
    "    y_train_nulls[\"Dealer_Listing_Price\"].min(),\n",
    "    y_train_nulls[\"Dealer_Listing_Price\"].max(),\n",
    ")\n",
    "\n",
    "\n",
    "# Set plot labels and title for the second subplot\n",
    "\n",
    "axes[1].set_xlabel(\"Dealer_Listing_Price\")\n",
    "\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].set_title(\"Distribution before Dropping Nulls\")\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# Plot counts after Group Trim-Price Imputaions\n",
    "sns.countplot(x=y_train[\"Vehicle_Trim\"], ax=axes[0])\n",
    "axes[0].tick_params(axis=\"x\", rotation=90, labelsize=9)  # Rotate and adjust label size\n",
    "\n",
    "# Add proportion labels for each class\n",
    "total_after_dropping_nulls = len(y_train[\"Vehicle_Trim\"].dropna())\n",
    "for p in axes[0].patches:\n",
    "    height = p.get_height()\n",
    "    axes[0].text(\n",
    "        p.get_x() + p.get_width() / 2,\n",
    "        height + 0.1,\n",
    "        f\"{height/total_after_dropping_nulls:.2%}\",\n",
    "        ha=\"center\",\n",
    "        rotation=45,  # Rotate the text\n",
    "    )\n",
    "\n",
    "# Set plot labels and title for the first subplot\n",
    "axes[0].set_xlabel(\"Vehicle_Trim\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution after Group Trim-Price Imputaions\")\n",
    "\n",
    "# Plot counts before Group Trim-Price Imputaions using y_train_nulls\n",
    "sns.countplot(x=y_train_nulls[\"Vehicle_Trim\"], ax=axes[1])\n",
    "axes[1].tick_params(axis=\"x\", rotation=90, labelsize=9)  # Rotate and adjust label size\n",
    "\n",
    "# Add proportion labels for each class\n",
    "total_before_dropping_nulls = len(y_train_nulls[\"Vehicle_Trim\"].dropna())\n",
    "for p in axes[1].patches:\n",
    "    height = p.get_height()\n",
    "    axes[1].text(\n",
    "        p.get_x() + p.get_width() / 2,\n",
    "        height + 0.1,\n",
    "        f\"{height/total_before_dropping_nulls:.2%}\",\n",
    "        ha=\"center\",\n",
    "        rotation=45, \n",
    "    )\n",
    "\n",
    "# Set plot labels and title for the second subplot\n",
    "axes[1].set_xlabel(\"Vehicle_Trim\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Distribution before Group Trim-Price Imputaions\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forgetting something? Yes, null values and outliers. Extensive outlier consideration will be omitted here due to time and knowing that logistic regression is marginally kinder to us in this regard due to log-odds considerations. The standard scaler also blunted some of this effect in theory. As for imputation we will see which method would be best used to cover nan values comparing against various baseline for permuting holes in the data. What we will do instead is get a more balanced sample is then end with SMOTE for additional minority classes consideration (else almost everything will be considered Limited or Premium Luxury for trim even if we see imputation was 'fair')\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea for imputation here is to see if there are massive swings in MSE when we impute the dataset going from one strategy to another. We shall use a modified set of matrices, dropping rows of X containing any nan and calling it a fully populated matrix alongside its target y. We then inflate this matrix with nans and assess impacts going from one strategy of imputation to another using the baseline RF regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_train_nulls)), np.any(np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random number generator\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "nan_rows_mask = np.any(np.isnan(X_train_nulls), axis=1)\n",
    "\n",
    "# Extract indices of rows with NaN values\n",
    "nan_rows_indices = np.where(nan_rows_mask)[0]\n",
    "\n",
    "# Drop rows with NaN values. There are artificially clean matrices for data (representing an abrasive dropping of all NaN entries\n",
    "# that introduce all hosts of problems if relied on as a defacto solution to handling missing data) we use it as a template\n",
    "# here to compare against imputation strategies\n",
    "X_full = X_train_nulls[~nan_rows_mask]\n",
    "y_full = y[~nan_rows_mask]\n",
    "\n",
    "\n",
    "def add_missing_values(X_full, y_full):\n",
    "    n_samples, n_features = X_full.shape\n",
    "\n",
    "    # Add missing values in 50% of the lines\n",
    "    missing_rate = 0.5\n",
    "    n_missing_samples = int(n_samples * missing_rate)\n",
    "\n",
    "    missing_samples = np.zeros(n_samples, dtype=bool)\n",
    "    missing_samples[:n_missing_samples] = True\n",
    "\n",
    "    rng.shuffle(missing_samples)\n",
    "    missing_features = rng.randint(0, n_features, n_missing_samples)\n",
    "    X_missing = X_full.copy()\n",
    "    X_missing[missing_samples, missing_features] = np.nan\n",
    "    y_missing = y_full.copy()\n",
    "\n",
    "    return X_missing, y_missing\n",
    "\n",
    "\n",
    "X_missing, y_missing = add_missing_values(X_full, y_full)\n",
    "\n",
    "print(f\"X_full nans -> {np.any(np.isnan(X_full))}\")\n",
    "print(f\"X_missing nans -> {np.any(np.isnan(X_missing))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# https://machinelearningmastery.com/multi-output-regression-models-with-python/\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# previously tuned on X_full\n",
    "base_estimators = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_jobs=-1),\n",
    "}\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Define imputation methods\n",
    "imputation_methods = {\n",
    "    \"Full Data\": None,\n",
    "    \"Zero Imputation\": SimpleImputer(\n",
    "        missing_values=np.nan, add_indicator=True, strategy=\"constant\", fill_value=0\n",
    "    ),\n",
    "    \"5-KNN Imputation\": KNNImputer(\n",
    "        missing_values=np.nan, add_indicator=True, n_neighbors=5\n",
    "    ),\n",
    "    \"15-KNN Imputation\": KNNImputer(\n",
    "        missing_values=np.nan, add_indicator=True, n_neighbors=15\n",
    "    ),\n",
    "    \"Mean Imputation\": SimpleImputer(\n",
    "        missing_values=np.nan, strategy=\"mean\", add_indicator=True\n",
    "    ),\n",
    "    \"Median Imputation\": SimpleImputer(\n",
    "        missing_values=np.nan, strategy=\"median\", add_indicator=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Evaluate each imputation method\n",
    "mses_car = {}\n",
    "stds_car = {}\n",
    "x_labels = []\n",
    "\n",
    "for name, imputer in imputation_methods.items():\n",
    "    for est_name, base_estimator in base_estimators.items():\n",
    "        pipeline_steps = [(\"base_estimator\", base_estimator)]\n",
    "\n",
    "        if imputer:\n",
    "            pipeline_steps.insert(0, (\"imputer\", imputer))\n",
    "\n",
    "        pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "        key = f\"{name} - {est_name}\"\n",
    "        print(key)\n",
    "\n",
    "        impute_scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_full if name == \"Full Data\" else X_missing,\n",
    "            y_full if name == \"Full Data\" else y_missing,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            cv=N_SPLITS,\n",
    "        )\n",
    "\n",
    "        mses_car[key] = -impute_scores.mean()\n",
    "        stds_car[key] = impute_scores.std()\n",
    "        x_labels.append(key)\n",
    "\n",
    "# Convert to numpy arrays for plotting\n",
    "mses_car = np.array(list(mses_car.values()))\n",
    "stds_car = np.array(list(stds_car.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MSEs with error bars (standard deviations)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate the min and max values of MSEs plus/minus one standard deviation\n",
    "min_mse = np.min(min(mses_car) - min(stds_car))\n",
    "max_mse = np.max(max(mses_car) + max(stds_car))\n",
    "\n",
    "# Plotting adjusted MSEs with error bars (standard deviations)\n",
    "bars = plt.bar(x_labels, mses_car, yerr=stds_car, capsize=10)\n",
    "\n",
    "plt.title(\"Comparison of Imputation Methods with Different Regressors\")\n",
    "plt.xlabel(\"Imputation Method - Regressor\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "\n",
    "# Adjust the y-axis limits\n",
    "plt.ylim(min_mse, max_mse)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though slight, we see that 5-KNN imputation after our dummy classification algo produced a better imputation scheme and will be used before SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(missing_values=np.nan, add_indicator=True, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = knn_imputer.fit_transform(X_train_nulls)\n",
    "X_test = knn_imputer.transform(X_test_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(X_train).any(), np.isnan(y).any(), np.isnan(X_test).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "# Plot counts before any resampling\n",
    "sns.countplot(x=y_train[\"Vehicle_Trim\"], ax=axes[0])\n",
    "axes[0].tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "axes[0].set_xlabel(\"Vehicle_Trim\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution before Resampling\")\n",
    "\n",
    "# Apply SMOTE oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_over, y_resampled_over = smote.fit_resample(X_train, y[:, :-1])\n",
    "\n",
    "# Plot counts after SMOTE oversampling\n",
    "sns.countplot(x=y_resampled_over[\"Vehicle_Trim\"], ax=axes[1])\n",
    "axes[1].tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "axes[1].set_xlabel(\"Vehicle_Trim\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Distribution after SMOTE Oversampling\")\n",
    "\n",
    "# Apply random undersampling\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled_under, y_resampled_under = under_sampler.fit_resample(X_train, y[:, :-1])\n",
    "\n",
    "# Plot counts after random undersampling\n",
    "sns.countplot(x=y_resampled_under[\"Vehicle_Trim\"], ax=axes[2])\n",
    "axes[2].tick_params(axis=\"x\", rotation=45, labelsize=9)\n",
    "axes[2].set_xlabel(\"Vehicle_Trim\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\"Distribution after Random Undersampling\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../data/py_training_X.csv\", X_train, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../data/py_test_X.csv\", X_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../data/py_test_id.csv\", y_test_id_nulls_raw, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    \"../data/py_y_enc_ft_names.csv\",\n",
    "    transformed_feature_names_y,\n",
    "    delimiter=\",\",\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(\n",
    "    \"../data/py_X_ft_names.csv\",\n",
    "    list(X_train_nulls_raw.columns),\n",
    "    delimiter=\",\",\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "\n",
    "np.savetxt(\"../data/py_y_train.csv\", y_train, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"../data/py_y_enc_train.csv\", y, delimiter=\",\", fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
